{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM结构和隐藏状态\n",
    "\n",
    "因为包含循环, 所以循环网络 (RNN) 可以在处理新输入的同时存储信息。长短记忆网络（LSTM）是一种特殊的循环网络，对 LSTM 来说，序列中的每个数据（例如，给定句子中的单词）都有一个相应的 *隐藏状态* $h_t$。该隐藏状态原则上可以包含序列当前结点之前的任一节点的信息；包括一些权重，短期和长期记忆成分的表示。\n",
    "\n",
    "因此，**LSTM的隐藏状态将根据输入语句中的每个新单词的变化而变化，可以使用隐藏状态来预测语句中下一个最可能的单词**，或者帮助确定语言模型中单词的类型，还有很多其他应用！\n",
    "\n",
    "### 练习库\n",
    "\n",
    "请注意，大多数练习的 notebook 都可以根据 [Github练习库](https://github.com/udacity/CVND_Exercises)中的说明在本地计算机上运行。\n",
    "\n",
    "\n",
    "## Pytorch中的LSTM\n",
    "\n",
    "要创建和训练 LSTM，必须要知道怎样构造 LSTM 的输入和隐藏状态。在 Pytorch中，可以把 LSTM 定义为：`lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=n_layers)`。\n",
    "\n",
    "PyTorch中的LSTM期望的输入都是3D张量，其维度定义如下\n",
    ">* `input_dim` = 输入中期望的序列的数量 (值为20表示有20个输入序列)\n",
    ">* `hidden_dim` = 隐藏状态的特征数；即各LSTM单元在每个时间步上的输出的数量。\n",
    ">* `n_layers ` = 循环层数；通常为1到3之间的值； 值为1表示每个LSTM单元有一个隐藏状态。 默认值为1。\n",
    "\n",
    "<img src='images/lstm_simple_ex.png' height=5 >\n",
    "    \n",
    "###  隐藏状态\n",
    "\n",
    "定义了 LSTM 的输入和隐藏状态的特征数，我们就可以在每个时间步中用它来获取输出和隐藏状态。 `out, hidden = lstm(input.view(1, 1, -1), (h0, c0))` \n",
    "\n",
    "LSTM的输入是 **`(input, (h0, c0))`**.\n",
    ">* `input` = 包含输入序列特征的张量；张量形状是（seq_len，batch，input_size）\n",
    ">* `h0` = 包含一个批次中每个元素的初始隐藏状态的张量\n",
    ">* `c0` = 包含一个批次中每个元素的初始记忆单元的张量\n",
    "\n",
    "\n",
    "`h0` 和 `c0` 的默认值是 0。其张量形状是： (n_layers, batch, hidden_dim).\n",
    "\n",
    "在本 notebook 示例中，这些概念将变得更加清晰。本 notebook 和 后续的 notebook 都是[此PyTorch LSTM教程](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#lstm-s-in-pytorch)的修改版。\n",
    "\n",
    "让我们举一个简单例子，例如我们想用 LSTM 处理一个句子 \"Giraffes in a field\"，将其输入 序列模型。我们的输入应是一个元素是单词的，\"1x4\"的行向量：\n",
    "\n",
    "\\begin{align}\\begin{bmatrix}\n",
    "   \\text{Giraffes  } \n",
    "   \\text{in  } \n",
    "   \\text{a  } \n",
    "   \\text{field} \n",
    "   \\end{bmatrix}\\end{align}\n",
    "\n",
    "在本例中，输入语句有4个单词，我们要设定每个时间步要生成多少个输出，例如设定每个LSTM单元生成 **3个隐藏状态值**。将LSTM中的循环层数保持为默认值1。\n",
    "\n",
    "隐藏状态和记忆单元的张量形状是（n_layers，batch，hidden_dim），在本例中，该张量形状是（1，1，3），即其输入是一批/单词序列（这一句话），循环层数为一层，生成3个隐藏状态值。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "接下来，让我们看一个LSTM的示例，该示例旨在查看输入是有4个值的序列（用数值类型，因为容易创建和跟踪），输出是3个值。这就是上面语句处理网络，建议你修改输入/隐藏状态的大小，以查看对LSTM结构有何影响！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(2) # so that random variables will be consistent and repeatable for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义一个简单的LSTM\n",
    "\n",
    "**关于隐藏状态和输出的数量的说明**\n",
    "\n",
    "除非你定义自己的LSTM，并通过在网络末端添加线性层来改变输出数量 例如，fc = nn.Linear(hidden_dim, output_dim)，否则隐藏状态数`hidden_dim`和输出的数量是相同的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# define an LSTM with an input dim of 4 and hidden dim of 3\n",
    "# this expects to see 4 values as input and generates 3 values as output\n",
    "input_dim = 4\n",
    "hidden_dim = 3\n",
    "lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim)  \n",
    "\n",
    "# make 5 input sequences of 4 random values each\n",
    "inputs_list = [torch.randn(1, input_dim) for _ in range(5)]\n",
    "print('inputs: \\n', inputs_list)\n",
    "print('\\n')\n",
    "\n",
    "# initialize the hidden state\n",
    "# (1 layer, 1 batch_size, 3 outputs)\n",
    "# first tensor is the hidden state, h0\n",
    "# second tensor initializes the cell memory, c0\n",
    "h0 = torch.randn(1, 1, hidden_dim)\n",
    "c0 = torch.randn(1, 1, hidden_dim)\n",
    "\n",
    "\n",
    "h0 = Variable(h0)\n",
    "c0 = Variable(c0)\n",
    "# step through the sequence one element at a time.\n",
    "for i in inputs_list:\n",
    "    # wrap in Variable \n",
    "    i = Variable(i)\n",
    "    \n",
    "    # after each step, hidden contains the hidden state\n",
    "    out, hidden = lstm(i.view(1, 1, -1), (h0, c0))\n",
    "    print('out: \\n', out)\n",
    "    print('hidden: \\n', hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以看到输出张量和隐藏张量的长度始终为3，因为在定义LSTM时，我们已经用`hidden_dim`指定了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一次全部\n",
    "\n",
    "for循环对于处理大的数据序列不是很有效，因此我们也可以 **一次处理所有这些输入。**\n",
    "\n",
    "1. 将所有输入序列连接到一个大小为 batch_size 的大张量中\n",
    "2. 定义隐藏状态的张量形状\n",
    "3. 获取输出和*最近*的隐藏状态（即根据序列中的最新的单词创建）\n",
    "\n",
    "由于初始隐藏状态不同，输出看起来可能略有不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn inputs into a tensor with 5 rows of data\n",
    "# add the extra 2nd dimension (1) for batch_size\n",
    "inputs = torch.cat(inputs_list).view(len(inputs_list), 1, -1)\n",
    "\n",
    "# print out our inputs and their shape\n",
    "# you should see (number of sequences, batch size, input_dim)\n",
    "print('inputs size: \\n', inputs.size())\n",
    "print('\\n')\n",
    "\n",
    "print('inputs: \\n', inputs)\n",
    "print('\\n')\n",
    "\n",
    "# initialize the hidden state\n",
    "h0 = torch.randn(1, 1, hidden_dim)\n",
    "c0 = torch.randn(1, 1, hidden_dim)\n",
    "\n",
    "# wrap everything in Variable\n",
    "inputs = Variable(inputs)\n",
    "h0 = Variable(h0)\n",
    "c0 = Variable(c0)\n",
    "# get the outputs and hidden state\n",
    "out, hidden = lstm(inputs, (h0, c0))\n",
    "\n",
    "print('out: \\n', out)\n",
    "print('hidden: \\n', hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 接下来：隐藏状态和门\n",
    "\n",
    "该 notebook 向你展示了PyTorch中LSTM的输入和输出的结构。在接下来的notebook 练习中，你将了解有关LSTM如何利用隐藏状态表示长期和短期记忆的更多信息。\n",
    "\n",
    "#### 词性\n",
    "在本课后面的 notebook 中，你将看到如何定义一个模型来标记词性（名词，动词，限定词），包括定义一个LSTM和一个线性层，以定义所需的输出大小，\n",
    "最后训练模型，以创建将一个将每个输入单词与词性相关联的分类得分分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
